# Implementation Plan: vLLM vs TGI vs Ollama Benchmarking

## Document Details

* **Project**: vLLM vs TGI vs Ollama Low-Latency Chat Benchmarking
* **Owner**: AI Platform Team
* **Created**: 2025-09-11
* **Version**: 2.0 (Script-Based Architecture)
* **Status**: Migration to Script-Based Implementation
* **Current Phase**: Day 4 - Visualization & Reporting (Benchmarking Core Complete)

---

## Executive Summary

This project provides a comprehensive benchmarking solution comparing three leading AI inference engines (vLLM, TGI, Ollama) for low-latency chat applications. After initial notebook development, we've pivoted to a **script-based architecture** for better maintainability, automation, and production readiness.

---

## ğŸ”„ Architecture Evolution

### Phase 1: Notebook Development (COMPLETED âœ…)
**Timeline**: September 11, 2025  
**Status**: Complete prototype with lessons learned

#### What Was Built
- **Interactive Jupyter Notebook**: 6-section comprehensive benchmarking workflow
- **Utility Modules**: Clean separation with `env_utils.py`, `api_clients.py`, `benchmark_utils.py`, `visualization_utils.py`
- **Infrastructure Integration**: Helm charts for vLLM, TGI, Ollama with anti-affinity rules
- **Comprehensive Features**: TTFT measurement, load testing, interactive visualizations

#### Key Achievements
- âœ… Validated technical approach and methodology
- âœ… Built complete utility frameworks for all functionality
- âœ… Demonstrated end-to-end benchmarking capability
- âœ… Created production-ready Helm infrastructure

#### Lessons Learned
- **Import Complexity**: Jupyter import caching and cell state management issues
- **User Experience**: CLI approach will be simpler and more maintainable
- **Production Readiness**: Scripts integrate better with CI/CD and automation
- **Version Control**: Standard Python files have better git workflow

### Phase 2: Script Migration (CURRENT ğŸš§)
**Timeline**: September 11-15, 2025  
**Status**: In Progress - Architecture Design Complete

#### Migration Strategy
**Preserve**: All working functionality and technical approaches  
**Improve**: User experience, maintainability, and production readiness  
**Add**: CLI interface, configuration system, better error handling

---

## ğŸ¯ Current Implementation Plan

### Week 1: Core Migration (September 11-15)

#### Day 1: Foundation âœ… COMPLETED
- [x] **Migration Planning**: Documented transition strategy
- [x] **Architecture Design**: CLI-based design with modular structure
- [x] **Project Structure**: Created new directory layout

#### Day 2: Core Modules âœ… COMPLETED
- [x] **Main CLI Script**: Create `vllm_benchmark.py` - the primary entry point
  - [x] Argument parsing with subcommands (`quick`, `standard`, `stress`, `custom`)
  - [x] YAML configuration file loading and validation
  - [x] Rich console initialization with consistent styling
  - [x] Service discovery orchestration and validation
  - [x] Test execution workflow management (basic framework)
  - [x] Results aggregation and output coordination (basic framework)
  - [x] Error handling and graceful exit strategies
  - [x] Progress reporting and user feedback
  - [ ] Logging configuration and file output
  - [ ] Infrastructure validation integration
- [x] **Service Discovery**: Migrate enhanced service discovery to `src/service_discovery.py`
  - [x] OpenShift route detection with TLS auto-detection
  - [x] Kubernetes ingress discovery
  - [x] NodePort service discovery with external IP resolution
  - [x] Manual URL override system
  - [x] Multiple health endpoint testing (`/health`, `/api/tags`, `/v1/models`)
  - [ ] Infrastructure validation script integration
- [x] **API Clients**: Migrate unified API client system to `src/api_clients.py`
  - [x] ServiceType enum and data classes (ChatMessage, GenerationRequest, TokenMetrics)
  - [x] BaseAPIClient with common functionality
  - [x] vLLMClient with OpenAI-compatible streaming API
  - [x] TGIClient with HuggingFace streaming API
  - [x] OllamaClient with chat/generate endpoints
  - [x] UnifiedAPIClient for concurrent testing across all services
  - [x] Helper functions for request creation
- [x] **Configuration System**: Implement YAML configuration loading in `src/config.py`
  - [x] Configuration dataclasses with validation
  - [x] Default configuration merging
  - [x] YAML file loading and parsing
  - [ ] Configuration file discovery (local, user, system)
  - [ ] Schema validation with helpful error messages

#### Day 3: Benchmarking Core âœ… COMPLETED
**Status**: All tasks completed - Production-ready benchmarking engine delivered  
**Commit**: `77d572f` - 11 files changed, 3,446 insertions(+)

- [x] **TTFT Testing**: Migrate TTFT measurement to `src/benchmarking.py`
  - [x] TTFTBenchmark class with streaming-based measurement
  - [x] Sub-millisecond accuracy via first token timestamp capture
  - [x] Statistical analysis across multiple iterations
  - [x] Target validation (100ms threshold)
  - [x] Beautiful Rich console output with progress tracking
- [x] **Load Testing**: Migrate concurrent load testing capabilities
  - [x] BenchmarkConfig dataclass with comprehensive parameters
  - [x] ServiceBenchmarkResult with detailed metrics
  - [x] LoadTestBenchmark with concurrent user simulation
  - [x] Progressive test scenarios (quick â†’ standard â†’ stress)
  - [x] Real user behavior patterns with think time
  - [x] Error classification and retry logic
- [x] **Metrics Collection**: Migrate statistical analysis to `src/metrics.py`
  - [x] P50/P95/P99 percentile calculations
  - [x] Success rate tracking and error analysis
  - [x] Target achievement validation
  - [x] Winner determination algorithms
  - [x] Comprehensive results aggregation

**Key Achievements:**
- ğŸ† **Enterprise-grade benchmarking suite** with scientific statistical analysis
- âš¡ **Sub-millisecond TTFT measurement** via streaming token capture
- ğŸ“Š **Comprehensive load testing** with real user behavior simulation
- ğŸ¯ **Winner determination algorithms** with multi-dimensional scoring
- ğŸ’ **Professional results display** with beautiful tables and insights
- ğŸ“ˆ **Export capabilities** to JSON/CSV with timestamped results
- ğŸ›¡ï¸ **Robust error handling** with graceful service degradation

#### Day 4: Visualization & Reporting
- [ ] **Chart Generation**: Migrate Plotly visualizations to `src/visualization.py`
  - [ ] BenchmarkVisualizer with consistent color schemes
  - [ ] TTFT comparison charts (box plots + bar charts)
  - [ ] Load test dashboard (4-panel comprehensive view)
  - [ ] Performance radar chart (multi-dimensional comparison)
  - [ ] Interactive features (zoom, hover, filtering)
  - [ ] Target lines and statistical annotations
- [ ] **Report Generation**: Create HTML/PDF reporting in `src/reporting.py`
  - [ ] Summary report generation with automated insights
  - [ ] Executive summary with key findings
  - [ ] Winner identification and recommendations
  - [ ] Professional HTML report templates
  - [ ] Chart embedding and styling
- [ ] **Export Formats**: JSON, CSV, and chart exports
  - [ ] Raw JSON results export
  - [ ] Processed CSV metrics export
  - [ ] Interactive HTML charts
  - [ ] Static PNG/SVG chart exports
- [ ] **Complete Testing**: Full workflow validation

#### Day 5: Critical Features & Polish
- [ ] **Infrastructure Integration**: Complete integration features
  - [ ] Infrastructure validation script execution
  - [ ] Anti-affinity rule validation
  - [ ] Resource availability checking
  - [ ] Kubernetes/OpenShift native discovery
- [ ] **Error Handling**: Robust error handling and recovery
  - [ ] Graceful degradation when services unavailable
  - [ ] Comprehensive error classification
  - [ ] Retry logic with exponential backoff
  - [ ] Clear debugging information and troubleshooting guides
- [ ] **Progress Reporting**: Real-time feedback during long operations
  - [ ] Progress bars for service discovery
  - [ ] Live updates during load testing
  - [ ] Status tables for health checks
  - [ ] Beautiful console output with Rich
- [ ] **Configuration Templates**: Create example config files
  - [ ] Default configuration (config/default.yaml)
  - [ ] Quick test configuration (config/quick-test.yaml)
  - [ ] Stress test configuration (config/stress-test.yaml)
  - [ ] Custom scenario examples
- [ ] **Documentation**: Update README and usage guides
- [ ] **Final Validation**: Production readiness check

---

## ğŸ—ï¸ New Architecture Overview

### Directory Structure
```
vllm-notebooks/
â”œâ”€â”€ vllm_benchmark.py          # ğŸ¯ Main CLI script
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml           # Default benchmark configuration
â”‚   â”œâ”€â”€ quick-test.yaml        # Quick latency test
â”‚   â”œâ”€â”€ stress-test.yaml       # Comprehensive stress test
â”‚   â””â”€â”€ examples/              # Configuration examples
â”œâ”€â”€ src/                       # ğŸ“¦ Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service_discovery.py   # Service discovery & health checks
â”‚   â”œâ”€â”€ api_clients.py         # Unified API clients (vLLM/TGI/Ollama)
â”‚   â”œâ”€â”€ benchmarking.py        # TTFT and load testing
â”‚   â”œâ”€â”€ metrics.py             # Statistical analysis
â”‚   â”œâ”€â”€ visualization.py       # Chart generation
â”‚   â””â”€â”€ reporting.py           # HTML/PDF report generation
â”œâ”€â”€ data/                      # ğŸ“ Test data (preserved)
â”‚   â””â”€â”€ prompts.txt            # Curated test prompts
â”œâ”€â”€ results/                   # ğŸ“Š Output directory
â”‚   â”œâ”€â”€ raw/                   # Raw benchmark data (JSON)
â”‚   â”œâ”€â”€ processed/             # Processed metrics (CSV)
â”‚   â”œâ”€â”€ reports/               # HTML/PDF reports
â”‚   â”œâ”€â”€ charts/                # Generated visualizations
â”‚   â””â”€â”€ logs/                  # Execution logs
â”œâ”€â”€ helm/                      # âš™ï¸ Infrastructure (preserved)
â”‚   â”œâ”€â”€ vllm/
â”‚   â”œâ”€â”€ tgi/
â”‚   â””â”€â”€ ollama/
â”œâ”€â”€ scripts/                   # ğŸ”§ Helper scripts (preserved)
â”‚   â””â”€â”€ infrastructure-validation.sh
â”œâ”€â”€ notebooks/                 # ğŸ““ Legacy (for reference)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Dockerfile                 # Container support
â””â”€â”€ README.md                  # Updated documentation
```

### Core Components

#### 1. Main Script (`vllm_benchmark.py`)
```bash
# Main script entry point with multiple modes
./vllm_benchmark.py [COMMAND] [OPTIONS]

# Quick latency test (5 minutes) - TTFT focus
./vllm_benchmark.py quick
  # â†’ Uses config/quick-test.yaml
  # â†’ 3 services, 10 iterations, basic metrics
  # â†’ Output: TTFT comparison chart + summary

# Standard comprehensive test (30 minutes)  
./vllm_benchmark.py standard
  # â†’ Uses config/default.yaml
  # â†’ All test scenarios, statistical analysis
  # â†’ Output: Full dashboard + HTML report

# Stress test for production validation (60+ minutes)
./vllm_benchmark.py stress --duration 60 --users 50
  # â†’ Uses config/stress-test.yaml
  # â†’ High concurrent load, reliability testing
  # â†’ Output: Load dashboard + performance report

# Custom configuration with specific parameters
./vllm_benchmark.py custom --config my-test.yaml --output-dir ./my-results
  # â†’ User-defined test scenarios
  # â†’ Flexible configuration override
  # â†’ Custom output location

# Additional utility commands
./vllm_benchmark.py validate-infra    # Run infrastructure validation
./vllm_benchmark.py discover-services # Show discovered service URLs
./vllm_benchmark.py generate-config   # Create example configuration files

# Global options available for all commands
--config PATH           # Custom configuration file
--output-dir PATH       # Results output directory  
--verbose              # Detailed logging
--no-charts            # Skip chart generation
--format json|html|csv # Output format preference
--services vllm,tgi    # Test specific services only
--dry-run              # Validate config without running tests
```

#### 2. Configuration System (`config/*.yaml`)
```yaml
benchmark:
  name: "vLLM vs TGI vs Ollama"
  description: "Low-latency chat benchmarking"

services:
  namespace: "vllm-benchmark"
  manual_urls:  # Optional override
    vllm: "https://vllm-route.apps.cluster.com"
    tgi: "https://tgi-route.apps.cluster.com"
    ollama: "https://ollama-route.apps.cluster.com"

test_scenarios:
  ttft:
    enabled: true
    iterations: 5
    target_ms: 100
  load_tests:
    - name: "quick_latency"
      concurrent_users: 5
      duration_seconds: 30
      target_p95_ms: 500
```

#### 3. Modular Components
- **Service Discovery**: Enhanced route/ingress discovery with fallbacks
- **API Clients**: Unified interface for vLLM/TGI/Ollama APIs
- **Benchmarking**: TTFT measurement and concurrent load testing
- **Metrics**: Statistical analysis (P50/P95/P99) and target validation
- **Visualization**: Interactive Plotly charts and dashboards
- **Reporting**: Professional HTML reports and CSV exports

---

## ğŸ¯ Migration Benefits

### For Users
âœ… **Simpler Execution**: Single command instead of notebook cells  
âœ… **Configuration Flexibility**: YAML configs for different scenarios  
âœ… **Better Results**: Clean file outputs and professional reports  
âœ… **CI/CD Ready**: Easy integration into automated pipelines  

### For Developers
âœ… **Standard Python**: No Jupyter complexities or import issues  
âœ… **Better Testing**: Unit tests for all components  
âœ… **Modular Design**: Clean separation of concerns  
âœ… **Version Control**: Better git workflow and collaboration  

### For Operations
âœ… **Automation**: Can be scheduled, containerized, and scripted  
âœ… **Monitoring**: Standard logging and error handling  
âœ… **Scalability**: Easier to run multiple tests concurrently  
âœ… **Integration**: Works with existing Python tooling  

---

## ğŸ“‹ Success Criteria

### Technical Requirements
- [ ] **Functionality Preservation**: All notebook features migrate successfully
- [ ] **Performance**: Same or better benchmark accuracy and reliability
- [ ] **Usability**: CLI interface is intuitive and well-documented
- [ ] **Configuration**: YAML configs cover all use cases
- [ ] **Output Quality**: Reports and visualizations are professional-grade

### Operational Requirements
- [ ] **Error Handling**: Robust error handling with clear messages
- [ ] **Logging**: Comprehensive logging for debugging and monitoring
- [ ] **Documentation**: Clear usage guides and examples
- [ ] **Testing**: Unit tests for core functionality
- [ ] **Containerization**: Docker support for deployment

### User Experience Requirements
- [ ] **Quick Start**: Users can run benchmarks with single command
- [ ] **Customization**: Easy configuration for different scenarios
- [ ] **Results**: Clear, actionable insights and recommendations
- [ ] **Troubleshooting**: Good error messages and debugging support

---

## ğŸš€ Implementation Status

### Completed âœ…
- Migration planning and architecture design
- Directory structure and main script framework
- Enhanced service discovery logic (from notebook work)
- Comprehensive utility functions (from notebook modules)
- Infrastructure validation and Helm charts
- **Complete benchmarking core implementation** (`src/benchmarking.py`)
- **Advanced metrics and statistical analysis** (`src/metrics.py`)
- **Production-ready CLI interface** with full workflow integration
- **YAML configuration system** with multiple presets
- **Enhanced API clients** with streaming support and error handling
- **Results export system** (JSON/CSV with timestamped outputs)

### In Progress ğŸš§
- Visualization and reporting system (Day 4 target)

### Pending ğŸ“‹
- Interactive Plotly charts and dashboards
- HTML/PDF report generation
- Documentation updates and examples
- Final testing and validation

---

## ğŸš¨ Critical Features Missing from Original Plan

### Features That Were Underspecified

#### 1. **Advanced Service Discovery (CRITICAL)**
**Missing**: Multi-layer discovery strategy we actually implemented
- âœ… **Built**: OpenShift routes â†’ Kubernetes ingress â†’ NodePort â†’ Internal fallback
- âœ… **Built**: TLS auto-detection for HTTPS/HTTP
- âœ… **Built**: External IP resolution for NodePort services
- âœ… **Built**: Manual URL override system
- âŒ **Plan**: Only mentioned "service discovery" generically

#### 2. **Streaming API Integration (CRITICAL)**
**Missing**: Sophisticated streaming implementation for TTFT
- âœ… **Built**: Sub-millisecond TTFT measurement via streaming APIs
- âœ… **Built**: First token timestamp capture across all three engines
- âœ… **Built**: Different streaming formats (SSE, JSON lines, custom)
- âœ… **Built**: Async generators for real-time token processing
- âŒ **Plan**: Only mentioned "TTFT measurement" without streaming details

#### 3. **Statistical Analysis Framework (CRITICAL)**
**Missing**: Comprehensive statistical engine we built
- âœ… **Built**: P50/P95/P99 percentile calculations with confidence intervals
- âœ… **Built**: Target validation against performance thresholds
- âœ… **Built**: Winner determination algorithms with multiple criteria
- âœ… **Built**: Success rate tracking and error classification
- âŒ **Plan**: Only mentioned "statistical analysis" without specifics

#### 4. **Interactive Visualization System (CRITICAL)**
**Missing**: Three distinct chart types with professional features
- âœ… **Built**: TTFT comparison (box plots + bar charts + target lines)
- âœ… **Built**: Load test dashboard (4-panel comprehensive view)
- âœ… **Built**: Performance radar (5-dimensional comparison)
- âœ… **Built**: Interactive features (zoom, hover, filtering)
- âœ… **Built**: Consistent color schemes and professional styling
- âŒ **Plan**: Only mentioned "chart generation" generically

#### 5. **Infrastructure Integration (CRITICAL)**
**Missing**: Deep Kubernetes/OpenShift integration
- âœ… **Built**: Infrastructure validation script execution
- âœ… **Built**: Anti-affinity rule validation
- âœ… **Built**: Resource availability checking
- âœ… **Built**: Native kubectl/oc command integration
- âŒ **Plan**: Mentioned but not detailed

#### 6. **Error Handling & UX (CRITICAL)**
**Missing**: Sophisticated error handling and user experience
- âœ… **Built**: Graceful degradation when services unavailable
- âœ… **Built**: Comprehensive error classification with specific messages
- âœ… **Built**: Rich console output with progress bars and status tables
- âœ… **Built**: Real-time feedback during long operations
- âœ… **Built**: Clear debugging information and troubleshooting guides
- âŒ **Plan**: Only mentioned "error handling" without UX focus

### Newly Added to Plan âœ…

All these missing features have now been **properly detailed** in the updated implementation plan with specific sub-tasks to ensure nothing is lost during migration.

---

## ğŸ¯ Next Steps

### Immediate (Today)
1. **Complete Core Modules**: Finish migrating service discovery and API clients
2. **Configuration System**: Implement YAML config loading and validation
3. **Basic Workflow**: Get end-to-end script execution working

### This Week
1. **Feature Migration**: Complete benchmarking and metrics migration
2. **Visualization**: Migrate chart generation and reporting
3. **Testing**: Validate all functionality works correctly
4. **Documentation**: Update all docs for script approach

### Next Week
1. **Polish**: Error handling, logging, edge cases
2. **Examples**: Configuration templates and usage examples
3. **Containerization**: Docker support and deployment guides
4. **User Validation**: Test with real scenarios and gather feedback

---

**This implementation plan will be updated as development progresses.**
