benchmark:
  name: "vLLM vs TGI vs Ollama - Quick Test"
  description: "Fast latency test for demo purposes"
  model: "Qwen/Qwen2.5-7B"

services:
  namespace: "vllm-benchmark"
  manual_urls:
    # Set these for quick testing when automatic discovery fails
    # vllm: "https://vllm-route.apps.cluster.com"
    # tgi: "https://tgi-route.apps.cluster.com"
    # ollama: "https://ollama-route.apps.cluster.com"
  timeout_seconds: 15
  health_check_retries: 2

test_scenarios:
  ttft:
    enabled: true
    iterations: 3
    target_ms: 100
    warmup_requests: 1
    max_tokens: 30
    temperature: 0.7

  load_tests:
    - name: "demo_test"
      concurrent_users: 3
      duration_seconds: 15
      target_p95_ms: 500
      ramp_up_seconds: 2
      max_tokens: 100
      temperature: 0.7

output:
  directory: "results"
  save_raw_data: true
  generate_charts: true
  generate_report: true
