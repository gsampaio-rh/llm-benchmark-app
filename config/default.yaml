benchmark:
  name: "vLLM vs TGI vs Ollama - Default"
  description: "Standard benchmarking configuration"
  model: "Qwen/Qwen2.5-7B"

services:
  namespace: "vllm-benchmark"
  manual_urls:
    # Uncomment and set these if automatic discovery fails
    # vllm: "https://vllm-route.apps.cluster.com"
    # tgi: "https://tgi-route.apps.cluster.com"
    # ollama: "https://ollama-route.apps.cluster.com"
  timeout_seconds: 30
  health_check_retries: 3

test_scenarios:
  ttft:
    enabled: true
    iterations: 5
    target_ms: 100
    warmup_requests: 2
    max_tokens: 50
    temperature: 0.7

  load_tests:
    - name: "quick_latency"
      concurrent_users: 5
      duration_seconds: 30
      target_p95_ms: 500
      ramp_up_seconds: 5
      max_tokens: 256
      temperature: 0.7

    - name: "standard_load"
      concurrent_users: 25
      duration_seconds: 60
      target_p95_ms: 1000
      ramp_up_seconds: 10
      max_tokens: 256
      temperature: 0.7

output:
  directory: "results"
  save_raw_data: true
  generate_charts: true
  generate_report: true
