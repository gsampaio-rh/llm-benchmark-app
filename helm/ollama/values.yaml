# Default values for Ollama
# This chart deploys Ollama as a third competitor to vLLM and TGI

# Override the name of the chart
nameOverride: ""
fullnameOverride: ""

# Namespace override (default: release namespace)
namespace: ""

# Number of Ollama replicas
replicaCount: 1

# Container image configuration
image:
  repository: docker.io/ollama/ollama
  tag: "latest"
  pullPolicy: IfNotPresent

# Image pull secrets for private registries
imagePullSecrets: []

# Configuration for this Ollama instance
config:
  name: "ollama-competitor"
  description: "Ollama deployment for vLLM and TGI comparison"

# Ollama server configuration
ollama:
  # Model to serve (will be pulled automatically)
  model: "qwen2.5:7b"
  
  # Server port
  port: 11434
  
  # Ollama server arguments and configuration
  args: []
  
  # Model pull configuration
  modelPull:
    enabled: true
    # Additional models to pre-pull
    additionalModels: []
    # - "qwen2.5:0.5b"
    # - "llama3.1:8b"
  
  # Additional environment variables
  env:
    - name: OLLAMA_HOST
      value: "0.0.0.0:11434"
    - name: OLLAMA_ORIGINS
      value: "*"
    - name: OLLAMA_KEEP_ALIVE
      value: "5m"
    - name: OLLAMA_NUM_PARALLEL
      value: "4"
    - name: OLLAMA_MAX_LOADED_MODELS
      value: "1"
    # Debug logging
    - name: OLLAMA_DEBUG
      value: "1"
  
  # Additional volume mounts
  additionalVolumeMounts: []
  
  # Additional volumes
  additionalVolumes: []

# HuggingFace configuration (for compatible models)
huggingface:
  # HuggingFace token for accessing gated models (not typically needed for Ollama)
  token: ""

# Service configuration
service:
  type: ClusterIP
  port: 11434
  # nodePort: 30434  # Only used if type is NodePort

# OpenShift Route configuration
route:
  enabled: true
  host: ""
  path: ""
  annotations: {}
  tls: {}

# Storage configuration
storage:
  enabled: true
  size: 100Gi  # Larger size for Ollama models
  accessModes:
    - ReadWriteOnce
  storageClass: ""  # Use default storage class
  shmSize: 2Gi  # Shared memory size for /dev/shm
  selector: {}

# Resource configuration
resources:
  limits:
    cpu: "4"
    memory: 32Gi  # More memory for Ollama model loading
    nvidia.com/gpu: 1  # GPU for better performance
  requests:
    cpu: "2"
    memory: 16Gi
    nvidia.com/gpu: 1

# Health check configuration
healthCheck:
  enabled: true
  readiness:
    path: /api/tags
    initialDelaySeconds: 90  # Increased to allow model loading
    periodSeconds: 10
    timeoutSeconds: 10       # Increased timeout
    failureThreshold: 5      # More retries
  liveness:
    path: /api/tags
    initialDelaySeconds: 180 # Increased for initial model loading
    periodSeconds: 30
    timeoutSeconds: 15       # Increased timeout
    failureThreshold: 3
  startup:
    path: /api/tags
    initialDelaySeconds: 60  # Start checking after init container
    periodSeconds: 20        # Check more frequently
    timeoutSeconds: 15       # Increased timeout
    failureThreshold: 60     # Allow up to 20 minutes for full startup

# Monitoring configuration
monitoring:
  enabled: false
  port: 11434
  path: /metrics
  serviceMonitor:
    enabled: false
    interval: 30s
    scrapeTimeout: 10s
    labels: {}

# Service account configuration
serviceAccount:
  create: true
  name: ""
  annotations:
    # For OpenShift SCC binding
    serviceaccounts.openshift.io/scc: anyuid

# Pod security context
podSecurityContext:
  runAsNonRoot: false  # Ollama containers often need root
  fsGroup: 0

# Container security context
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# Pod annotations
podAnnotations: {}

# Node selector
# Set to empty {} if you want to allow scheduling on any node
# Or use nvidia.com/gpu.present: "true" to require GPU nodes
nodeSelector:
  nvidia.com/gpu.present: "true"

# Tolerations - REQUIRED for GPU nodes with taints
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
  # Uncomment if GPU nodes also have PreferNoSchedule taint
  # - key: "nvidia.com/gpu"
  #   operator: "Exists"
  #   effect: "PreferNoSchedule"

# Affinity - spread across different nodes and avoid vLLM/TGI
affinity:
  podAntiAffinity:
    # Prefer to avoid nodes with vLLM pods
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: vllm
        topologyKey: kubernetes.io/hostname
    # Prefer to avoid nodes with TGI pods
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: tgi
        topologyKey: kubernetes.io/hostname
    # Also prefer to spread Ollama instances across nodes
    - weight: 50
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: ollama
        topologyKey: kubernetes.io/hostname

# Pod disruption budget
podDisruptionBudget:
  enabled: false
  # minAvailable: 1
  # maxUnavailable: 1
