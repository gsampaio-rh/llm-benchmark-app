# ğŸš€ Universal LLM Engine Benchmarking Tool

A Python-based benchmarking framework for evaluating the runtime performance of **Ollama**, **vLLM**, and **HuggingFace Text Generation Inference (TGI)**.

**âœ¨ Features beautiful, guided Python scripts with step-by-step instructions and rich visual feedback.**

---

**ğŸ“Š Current Status:** Phase 1 Complete + Phase 2 In Progress (52.4% metrics coverage)  
**ğŸ‰ Latest:** Parallel execution mode, Kubernetes/OpenShift integration, enhanced 8-column metrics dashboard  
**ğŸš€ New:** Real token-by-token streaming from all engines, scenario-based benchmarking

## ğŸ—ï¸ Quick Start

### Prerequisites
- Python 3.11+
- At least one LLM engine running (Ollama, vLLM, or TGI)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd llm-benchmark-app

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

## ğŸ¯ How to Use

This tool provides **interactive Python scripts** instead of CLI commands. Each script guides you through the process with beautiful visualizations.

### 1ï¸âƒ£ Check Engine Health
Verify that your LLM engines are running and accessible:
```bash
python scripts/check_engines.py
```
**What it does:** Tests connectivity, measures response times, lists available models

### 2ï¸âƒ£ Test a Single Request
Send a test prompt to any engine and model:
```bash
python scripts/test_request.py
```
**What it does:** Interactive model selection, sends request, shows detailed metrics, auto-exports to JSON

### 3ï¸âƒ£ Run Full Benchmark
Execute comprehensive benchmarks across engines:
```bash
python scripts/run_benchmark.py
```
**What it does:** Configure test parameters, run multiple requests, display results, auto-export to JSON

### 4ï¸âƒ£ Creative Writing Benchmark
Run scenario-based benchmark for story generation and creative writing:
```bash
python scripts/benchmark_creative_writing.py
```
**What it does:** 
- Tests short prompt â†’ long completion scenarios
- **Parallel execution** - test all engines simultaneously (3x faster!)
- Real-time token streaming with multi-column view
- Enhanced metrics dashboard with p95/p99 percentiles
- Auto-exports comprehensive results (JSON/CSV/Markdown)

### 5ï¸âƒ£ Discover Models
Explore available models across all engines:
```bash
python scripts/discover_models.py
```
**What it does:** Scan engines, list models by family, show availability

## ğŸ“š Complete Documentation

See **[scripts/README.md](scripts/README.md)** for detailed documentation on each script.

## ğŸŒŸ Key Features & Highlights

### âš¡ Parallel Execution Mode
Test all engines **simultaneously** instead of sequentially:
- **3x faster** benchmark completion
- Multi-column real-time view showing all engines side-by-side
- Live token streaming from all engines at once
- Immediate visual performance comparison

### ğŸ¬ Real Token-by-Token Streaming
- **True streaming** from all engines (Ollama, vLLM, TGI) - not simulated!
- Real-time dashboard updates as tokens arrive
- Accurate TTFT (Time to First Token) measurement
- Streaming smoothness tracking (inter-token latency)

### ğŸ³ Kubernetes/OpenShift Integration
- Auto-discovers pod metadata from OpenShift routes
- Displays CPU/Memory/GPU resource allocation
- Helm chart-based pod identification
- Shows which backend instance you're testing
- Zero-configuration setup

### ğŸ“Š Enhanced Metrics Dashboard
**8-column comprehensive view:**
- Throughput (avg Â± Ïƒ) - with variance tracking
- TTFT (avg Â· p95) - Time to First Token with percentiles
- Duration (avg Â· p95) - Total response time
- Inter-token latency - streaming smoothness
- Tokens/Response - consistency metric
- Token/Word ratio - tokenizer efficiency (color-coded)
- Total tokens generated
- Live percentile calculations (p50, p95, p99)

### ğŸ¯ Scenario-Based Benchmarking
- YAML-driven test configurations
- Pre-built scenarios for common use cases:
  - Short prompt â†’ Long completion (creative writing)
  - Long prompt â†’ Short completion (RAG/Q&A)
  - Long prompt â†’ Long completion (document analysis)
  - Short prompt â†’ Short completion (chat/interactive)
- Parameterized prompt templates
- Reproducible, version-controlled benchmarks

## âœ¨ What's Implemented

### Engine Connectivity âœ…
- **Ollama**: REST API integration with health checks and model discovery
- **vLLM**: OpenAI-compatible API support with streaming
- **TGI**: HuggingFace Inference API integration
- **Kubernetes/OpenShift Integration**: Auto-discovers pod metadata, CPU/Memory/GPU resources

### Interactive Scripts âœ…
- **check_engines.py** - Engine health and connectivity checker
- **discover_models.py** - Model discovery and exploration
- **test_request.py** - Single request tester with automatic metrics export
- **run_benchmark.py** - Comprehensive benchmark runner with real-time results
- **benchmark_creative_writing.py** - Scenario-based benchmark with parallel execution

### Real-Time Streaming âœ…
- **True token-by-token streaming** from all engines (not simulated!)
- Real-time dashboard updates as tokens arrive
- **Parallel execution mode** - test all engines simultaneously (3x faster)
- Multi-column view with side-by-side streaming
- Live metrics updates (10 Hz refresh rate)

### Scenario-Based Benchmarking âœ…
- **YAML-based scenario configurations**
- Pre-built scenarios (shortâ†’long, longâ†’short, longâ†’long, shortâ†’short)
- Parameterized prompt templates
- Parallel or sequential execution modes
- Comprehensive test case expansion

### Metrics Collection âœ…
- Raw engine metrics collection (100%)
- Standardized metrics parsing (52.4% coverage)
- Per-request runtime metrics (100% for Ollama & vLLM)
- **Enhanced 8-column metrics dashboard**:
  - Throughput (avg Â± Ïƒ) with variance tracking
  - TTFT (avg Â· p95) - Time to First Token
  - Duration (avg Â· p95) - Total response time
  - Inter-token latency (streaming smoothness)
  - Tokens/Response (consistency metric)
  - Token/Word ratio (tokenizer efficiency)
- JSON/CSV/Markdown export functionality
- Aggregate statistics (p50, p95, p99, mean, std dev)

### Infrastructure Monitoring âœ…
- **Pod metadata extraction** from OpenShift/Kubernetes
- CPU/Memory/GPU resource allocation display
- Helm chart-based pod discovery
- OpenShift route parsing
- Graceful degradation for local development

## ğŸ“Š Supported Metrics

### Per-Request Runtime Metrics (100% for Ollama & vLLM)
| Metric | Description | Engines |
|--------|-------------|---------|
| **Load Duration** | Setup time before prompt evaluation | Ollama, vLLM |
| **Prompt Eval Count** | Number of input tokens processed | All |
| **Prompt Eval Duration** | Time spent processing input tokens | Ollama, vLLM |
| **Prompt Token Rate** | Input tokens per second | Ollama, vLLM |
| **Response Token Count** | Number of output tokens generated | All |
| **Response Generation Time** | Time spent generating output | Ollama, vLLM |
| **Response Token Rate** | Output tokens per second | All |
| **Total Duration** | End-to-end request runtime | All |

### Latency Metrics (67% coverage)
| Metric | Description | Engines |
|--------|-------------|---------|
| **Time to First Token (TTFT)** | Latency before first token | Ollama, vLLM, TGI |
| **Inter-Token Latency** | Average time between tokens | Ollama, vLLM, TGI |
| **End-to-End Latency** | Total request duration | All |

### Aggregate Statistics
- **Percentiles**: p50, p95, p99 for all latency metrics
- **Variance**: Standard deviation (Ïƒ) for throughput consistency
- **Efficiency Metrics**: Token/word ratio, tokens per response
- **Reliability**: Success rate, error rate, timeout count

## ğŸ”§ Configuration

### Engine Configuration
Engine configurations are stored in `configs/engines/`:

```yaml
# configs/engines/ollama.yaml
name: "ollama"
engine_type: "ollama"
base_url: "http://localhost:11434"
timeout: 300
health_endpoint: "/api/tags"
```

### Scenario Configuration
Scenario configurations are stored in `configs/scenarios/`:

```yaml
# configs/scenarios/short_prompt_long_completion.yaml
scenario:
  name: "short_prompt_long_completion"
  description: "Creative writing expansion"
  use_case: "creative_writing"
  parallel_execution: true  # Run engines simultaneously (3x faster!)
  
  prompt:
    template: "Write a detailed story about {topic}"
    min_tokens: 5
    max_tokens: 20
    length_category: "short"
  
  completion:
    max_tokens: 500
    temperature: 0.7
    length_category: "long"
  
  test_cases:
    - topic: "a robot learning to paint"
    - topic: "time travel paradox"
    - topic: "life on Mars colony"
```

## ğŸ§ª Testing & Development

### Run Tests
```bash
# Run all tests
pytest

# Run unit tests only
pytest tests/unit/

# Run integration tests (requires running engines)
pytest tests/integration/

# Run with coverage
pytest --cov=src --cov-report=html
```

### Development Tools
```bash
# Format code
black src/ tests/ scripts/
isort src/ tests/ scripts/

# Type checking
mypy src/

# Linting
flake8 src/ tests/ scripts/
```

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ adapters/          # Engine-specific adapters (Ollama, vLLM, TGI)
â”œâ”€â”€ benchmarking/      # Benchmark execution & orchestration
â”‚   â”œâ”€â”€ benchmark_runner.py    # Core runner with parallel execution
â”‚   â”œâ”€â”€ live_dashboard.py      # Real-time metrics dashboard
â”‚   â””â”€â”€ target_selector.py     # Interactive target selection
â”œâ”€â”€ config/            # Configuration management
â”‚   â”œâ”€â”€ config_manager.py      # Engine config loader
â”‚   â”œâ”€â”€ scenario_loader.py     # YAML scenario loader
â”‚   â””â”€â”€ scenario_models.py     # Scenario data models
â”œâ”€â”€ core/              # Core framework components
â”‚   â”œâ”€â”€ connection_manager.py  # Engine connection pooling
â”‚   â””â”€â”€ metrics_collector.py   # Metrics collection
â”œâ”€â”€ models/            # Pydantic data models
â”‚   â”œâ”€â”€ engine_config.py       # Engine configuration models
â”‚   â””â”€â”€ metrics.py             # Metrics data models
â”œâ”€â”€ reporting/         # Export & reporting
â”‚   â””â”€â”€ export_manager.py      # JSON/CSV/Markdown export
â”œâ”€â”€ utils/             # Utility modules
â”‚   â””â”€â”€ k8s_metadata.py        # Kubernetes/OpenShift integration
â””â”€â”€ visualization/     # Terminal UI components
    â””â”€â”€ live_display.py        # Streaming visualization

scripts/               # ğŸŒŸ Interactive guided scripts
â”œâ”€â”€ check_engines.py           # Engine health checker
â”œâ”€â”€ discover_models.py         # Model discovery
â”œâ”€â”€ test_request.py            # Single request tester
â”œâ”€â”€ run_benchmark.py           # Basic benchmark runner
â”œâ”€â”€ benchmark_creative_writing.py  # Scenario benchmark with parallel execution
â”œâ”€â”€ demo_parallel_race.py      # Parallel streaming demo
â””â”€â”€ README.md                  # Scripts documentation

configs/
â”œâ”€â”€ engines/           # Engine configuration files
â”‚   â”œâ”€â”€ ollama.yaml
â”‚   â”œâ”€â”€ vllm.yaml
â”‚   â””â”€â”€ tgi.yaml
â””â”€â”€ scenarios/         # YAML scenario definitions
    â”œâ”€â”€ short_prompt_long_completion.yaml
    â”œâ”€â”€ long_prompt_short_completion.yaml
    â”œâ”€â”€ long_prompt_long_completion.yaml
    â””â”€â”€ short_prompt_short_completion.yaml

benchmark_results/     # Exported metrics and results
â””â”€â”€ run_YYYYMMDD_HHMMSS/
    â”œâ”€â”€ summary.json           # Cross-engine comparison
    â”œâ”€â”€ summary.csv            # Spreadsheet-ready
    â”œâ”€â”€ {engine}_results.json  # Per-engine complete data
    â”œâ”€â”€ {engine}_results.csv   # Per-engine tabular
    â””â”€â”€ report.md              # Human-readable summary

tests/
â”œâ”€â”€ unit/              # Unit tests (75+ tests, 100% passing)
â””â”€â”€ integration/       # Integration tests

helm/                  # Kubernetes/OpenShift Helm charts
â”œâ”€â”€ ollama/            # Ollama deployment
â”œâ”€â”€ vllm/              # vLLM deployment
â””â”€â”€ tgi/               # TGI deployment
```

## ğŸ“ License

Apache 2.0 License - see LICENSE file for details.

## ğŸ‘¨â€ğŸ’» Author

**Gabriel Sampaio** - gab@redhat.com

